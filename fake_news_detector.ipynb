{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "fake_df = pd.read_csv('Fake.csv')\n",
        "true_df = pd.read_csv('True.csv')\n",
        "\n",
        "#0 fake, 1  true\n",
        "fake_df['label'] = 0\n",
        "true_df['label'] = 1\n",
        "\n",
        "df = pd.concat([true_df, fake_df], ignore_index=True)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnKb1x3XO9pN",
        "outputId": "fcbf4bf7-486b-497c-e487-8affcb829b57"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44898"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower() # Lowercase\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "\n",
        "    tokens = text.split()\n",
        "    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(clean_tokens)\n",
        "\n",
        "df['text'] = df['title'] + \" \" + df['text']\n",
        "df['text'] = df['text'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "9C5HC_HIgw04"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "X = tokenizer(\n",
        "    text=df['text'].tolist(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=128,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    return_tensors='tf',\n",
        "    return_attention_mask=True\n",
        ")\n",
        "\n",
        "y = df['label'].values\n"
      ],
      "metadata": {
        "id": "fcZlHylOhIjK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "input_ids = X['input_ids'].numpy()\n",
        "attention_mask = X['attention_mask'].numpy()\n",
        "\n",
        "# Split the data\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, y, random_state=42, test_size=0.2)\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_mask, y, random_state=42, test_size=0.2)\n",
        "\n",
        "train_inputs.shape\n",
        "test_inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QItTFuY9hnhD",
        "outputId": "b792567d-5e99-4a78-8c5e-248a48fa982a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8980, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, from_pt=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "history = model.fit(\n",
        "    {'input_ids': train_inputs, 'attention_mask': train_masks},\n",
        "    train_labels,\n",
        "    validation_data=({'input_ids': test_inputs, 'attention_mask': test_masks}, test_labels),\n",
        "    epochs=1,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7G7QvQ9hxWb",
        "outputId": "a02ccfa9-2ebe-4436-9ef3-38dae136b369"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1123/1123 [==============================] - 1134s 945ms/step - loss: 0.0223 - accuracy: 0.9940 - val_loss: 0.0065 - val_accuracy: 0.9990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./saved_model')\n",
        "tokenizer.save_pretrained('./saved_model')\n",
        "\n",
        "print(\"saved_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9GrTzVaitdP",
        "outputId": "7f5baae2-f175-4df8-d711-1c7478bc8758"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkA5X1fwjuXx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}